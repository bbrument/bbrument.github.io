<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>RNb-NeuS</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel='icon' href='assets/icons/rnb.png' sizes='472x472'>

    <link href="neural/npm/bootstrap%405.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-GLhlTQ8iRABdZLl6O3oVMWSktQOp6b7In1Zl3/Jr59b6EGGoI1aFkw7cmDA6j6gD" crossorigin="anonymous">

    <script src="neural/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script id="MathJax-script" async="" src="../../npm/mathjax%403/es5/tex-mml-chtml.js"></script>
    <!-- <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,300,700' rel='stylesheet' type='text/css'> -->

    <link rel="stylesheet" href="js/dics/dics.original.css">
    <script src="js/dics/dics.original.js"></script>
    <script src="js/event_handler.js"></script>

    <link href="neural/css?family=IBM+Plex+Sans:300,400,500,600,700" rel="stylesheet">
    <link href="neural/css-1?family=Inconsolata:300,400,500,600,700" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="css/style_neural.css">
</head>

<body lang="en">
    <main role="main">

        <section class="jumbotron text-center" id="banner">
            <div class="container text-center" id="title">
                <h2 class="jumbotron-heading text-center" style="line-height: 35px;">
                    RNb-NeuS: Reflectance and Normal-based Multi-View </br> 3D Reconstruction
                </h2>
                
            </div>

            <div class="container" style="max-width: none; margin-top:10px;">

                <div class="row row-author" style="margin: 0 70px 0 70px; ">
                    <div class="col-md author">
                        <a class="link" href="https://bbrument.github.io/" target="_blank">Baptiste Brument</a><sup style="margin-right:10px">1,*</sup>
                        <a class="link" href="index.html" target="_blank">Robin Bruneau</a><sup>1,2,*</sup>
                    </div>   
                </div>
                <div class="col-md" style="margin-top: 0px;"></div>
                <div class="row row-author" style="margin: 0 70px 0 70px;">
					<div class="col-md author">
                        <a href="https://sites.google.com/view/yvainqueau/" target="_blank">Yvain Quéau</a><sup style="margin-right:10px">3</sup>
                        <a href="https://scholar.google.fr/citations?user=t9PRZ3AAAAAJ&hl=en" target="_blank">Jean Mélou</a><sup style="margin-right:10px">1</sup>
                        <a href="https://loutchoa.github.io/" target="_blank">François Lauze</a><sup style="margin-right:10px">2</sup>
                        <a href="https://www.irit.fr/~Jean-Denis.Durou/" target="_blank">Jean-Denis Durou</a><sup style="margin-right:10px">1</sup>
                        <a href="https://fr.linkedin.com/in/lilian-calvet-42b1a689" target="_blank">Lilian Calvet</a><sup>4</sup>
                    </div>
                </div>

                <div class="row" style="margin-top:0.5rem; margin-left: auto; margin-right: auto;">
                    <div class="col-md">
                        <div id="affiliation">
                            <sup>1</sup>IRIT, University of Toulouse, France &nbsp;&nbsp;&nbsp;&nbsp; 
                            <sup>2</sup>DIKU, University of Copenhagen, Denmark
                        </div>
                    </div>
                </div>
                <div class="row" style="margin-top:-1rem; margin-left: auto; margin-right: auto;">
                    <div class="col-md">
                        <div id="affiliation"> 
                            <sup>3</sup>GREYC, University of Caen, France &nbsp;&nbsp;&nbsp;&nbsp; 
                            <sup>4</sup>OR-X, Balgrist, University of Zurich, Switzerland
                        </div>
                    </div>
                </div>
            </div>

            <!--<div id="venue">
                IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024<br><br>
            </div>-->

            <div class="row">
                <div class="col-md">
                    <ul class="nav nav-pills">
                        <li style="padding-top: 10px;" class="space-between">
                            <a href="https://arxiv.org/abs/2312.01215/" class="hover-image-link" style="padding-top: 0;">
                            <image class="first-image" src="assets/icons/paper.png" width="60px"></image>
                            <image class="second-image" src="assets/icons/paper_hover.png" width="60px"></image>
                            <h3 style="font-size: 1.2rem; padding-top: 10px;"><strong>Paper</strong></h3>
                            </a>
                        </li>
                        <li style="padding-top: 10px;"  class="space-between">
                            <a href="https://github.com/bbrument/RNb-NeuS/" class="hover-image-link" >
                            <image class="first-image" src="assets/icons/github.png" width="60px"></image>
                            <image class="second-image" src="assets/icons/github_hover.png" width="60px"></image>
                            <h3 style="font-size: 1.2rem; padding-top: 10px;"><strong>Code</strong></h3>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        </section>
    </main>


    <div class="container">

        <hr style="border-top: 3px double;">

        <!-- Abstract -->
        <h5>Abstract</h5>
        <p style="text-align: justify; padding-bottom: 30px;">
            This paper introduces a versatile paradigm for integrating multi-view reflectance and normal maps acquired through photometric stereo. 
            Our approach employs a pixel-wise joint re-parameterization of reflectance and normal, considering them as a vector of radiances rendered 
            under simulated, varying illumination. This re-parameterization enables the seamless integration of reflectance and normal maps as input 
            data in neural volume rendering-based 3D reconstruction while preserving a single optimization objective. In contrast, recent multi-view 
            photometric stereo (MVPS) methods depend on multiple, potentially conflicting objectives. Despite its apparent simplicity, our proposed 
            approach outperforms state-of-the-art approaches in MVPS benchmarks across F-score, Chamfer distance, and mean angular error metrics. 
            Notably, it significantly improves the detailed 3D reconstruction of areas with high curvature or low visibility.
        </p>

        <!-- Overview -->
        <h5>Overview</h5>
        <p style="text-align: justify;">
            Overview of the proposed MVPS pipeline. The reflectance and normal maps provided for each view by PS are fused, by combining volume rendering with a pixel-wise re-parameterization of the inputs using physically-based rendering.
        </p>
        <div class="text-center;" style="padding-top: 30px; padding-bottom: 30px">
            <image src="assets/rnb_neus/images/pipeline.png" class="img-responsive" alt="intro">
        </div>
        <p style="text-align: justify;">
            The proposed approach aims to infer a 3D scene model by integrating reflectance and normal maps obtained from photometric stereo (PS) across multiple viewpoints and illuminations. 
            The input data, consisting of reflectance and normal values, is re-parameterized for 3D surface inference. 
            The objective function minimizes the difference between simulated intensity vectors derived from input data and volume rendering, 
            incorporating a regularization term on the signed distance function (SDF) of the surface. The reflectance and normal data are jointly 
            re-parameterized into radiance values using a linear Lambertian model, ensuring bijectivity. The volume rendering-based 3D reconstruction 
            employs a function to simulate intensity vectors, considering varying illumination. The optimization involves minimizing the photometric loss 
            while enforcing regularization on the SDF, leading to a consistent and single-objective formulation for 3D surface reconstruction.
            
        </p>

		
        <!-- Comparisons Videos -->
        <hr style="border-top: 3px double;">
        <h5>Comparisons</h5>

        <h6>Comparisons of state-of-the-art methods on DiLiGenT-MV</h6>
        <p style="text-align: justify;">
            We compare our approach with state-of-the-art methods on the DiLiGenT-MV dataset. 
            The quantitative comparisons are based on the F-score, Chamfer distance, and mean angular error metrics. 
            Our approach outperforms state-of-the-art methods across all metrics, demonstrating its effectiveness in 3D reconstruction 
            (cf. Section 5 in the paper).
            <br></br>
            We provide in a qualitative comparison of our results on DiLiGenT-MV, and compare them with the four most recent methods 
            from the literature, namely <a href="https://berk95kaya.github.io/UA_MVPS_project_page/">Kaya22</a>, 
            <a href="https://ywq.github.io/psnerf/">PS-NeRF</a>, 
            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_MVPSNet_Fast_Generalizable_Multi-view_Photometric_Stereo_ICCV_2023_paper.pdf">MVPSNet</a> 
            and <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Kaya_Multi-View_Photometric_Stereo_Revisited_WACV_2023_paper.pdf">Kaya23</a>. 
        </p>
        
        <div class="container;" style="padding-bottom: 10px;">
            <ul class="nav nav-tabs nav-fill nav-justified" id="video-changer">
                <li class="nav-item">
                    <a class="nav-link" onclick="changeVideo(0)">Bear</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link active" onclick="changeVideo(1)">Buddha</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" onclick="changeVideo(2)">Cow</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" onclick="changeVideo(3)">Pot2</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" onclick="changeVideo(4)">Reading</a>
                </li>
            </ul>
            
            <div>
                <video id="myVideo" width="100%" class="approach_video" controls="" autoplay="" muted="" loop="">
                    <source  src="assets/rnb_neus/videos/cmp/buddha.mp4" type="video/mp4">
                </video>
            </div>
        </div>



        <!-- Comparisons Images -->
        <!-- <hr style="border-top: 3px double;"> -->
        <h6>Image comparisons on DiLiGenT-MV</h6>
        <p style="text-align: justify;">
            In comparison with two state-of-the-art deep learning-based methods, namely <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_MVPSNet_Fast_Generalizable_Multi-view_Photometric_Stereo_ICCV_2023_paper.pdf">MVPSNet</a> 
            and <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Kaya_Multi-View_Photometric_Stereo_Revisited_WACV_2023_paper.pdf">Kaya23</a>, 
            the recovered geometry is overall more satisfactory and fine details are better preserved.
        </p>
        <div class="container;" style=" padding-bottom: 10px;">
            <ul class="nav nav-tabs nav-fill nav-justified" id="object-scale-recon">
                <li class="nav-item">
                    <a class="nav-link" onclick="objectSceneEvent(0)">Bear</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link active" onclick="objectSceneEvent(1)">Buddha</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" onclick="objectSceneEvent(2)">Cow</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" onclick="objectSceneEvent(3)">Pot2</a>
                </li>
				<li class="nav-item">
                    <a class="nav-link" onclick="objectSceneEvent(4)">Reading</a>
                </li>
            </ul>
            <div class="bordered-container">
            <div class="b-dics" style="width: 60%; font-weight: 600; margin: 0 auto;">
                <img src="assets/rnb_neus/images/cmp_methods/buddha/kaya.png" alt="Kaya et al. 23">
                <img src="assets/rnb_neus/images/cmp_methods/buddha/ours.png" alt="RNb-Neus (Ours)">
                <img src="assets/rnb_neus/images/cmp_methods/buddha/mvpsnet.png" alt="MVPSNet">
            </div>
            </div>
        </div>


        <!-- Comparisons Images -->
        <!-- <hr style="border-top: 3px double;"> -->
        <h6>NeuS (mono-illumination) vs Ours (multi-illumination)</h6>

        <p style="text-align: justify;">
            We propose an additional comparison of our method against the multi-view mono-illumination 3D reconstruction method 
            <a href="https://lingjie0206.github.io/papers/NeuS/">NeuS</a>. 
            While NeuS is not directly applicable in multi-view multi-light acquisition settings in theory, it may become feasible under certain conditions.
            This feasibility hinges on factors such as the number, spatial distribution and types of lighting conditions, and the object material properties. 
            One can leverage a heuristic method such as approximating input images captured under mono-illumination for each viewpoint by taking the median of pixel 
            intensities obtained under varying illumination (cf. supplementary material of the paper).
            <br></br>
            A qualitative comparison between the results of mono-illumination NeuS using this heuristic and the ones from our method 
            is provided. As can be seen, our proposed approach provides a much finer level of details. 
            In particular, mono-illumination NeuS requires a high number of viewpoints, with a drastic decline in the reconstruction quality when using 5 viewpoints. 
            On the contrary, our method shows stable results, only losing some fine details over concave areas.
        </p>

        <div class="container;" style=" padding-bottom: 10px; font-size: 0;">
            <ul class="nav nav-tabs nav-fill nav-justified" style="font-size: 16px;" id="modelChoice">
                <li class="nav-item">
                    <a class="nav-link" onclick="changeModel(0)">Bear</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link active" onclick="changeModel(1)">Buddha</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" onclick="changeModel(2)">Cow</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" onclick="changeModel(3)">Pot2</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" onclick="changeModel(4)">Reading</a>
                </li>
            </ul>
            <ul class="nav nav-tabs nav-fill nav-justified" id="viewChoice">
            <li class="nav-item" style="font-size: 16px;">
                <a class="nav-link active" onclick="changeView(0)">5 views</a>
            </li>
            <li class="nav-item" style="font-size: 16px;">
                <a class="nav-link" onclick="changeView(1)">10 views</a>
            </li>
            <li class="nav-item" style="font-size: 16px;">
                <a class="nav-link" onclick="changeView(2)">15 views</a>
            </li>
            <li class="nav-item" style="font-size: 16px;">
                <a class="nav-link" onclick="changeView(3)">20 views</a>
            </li>
        </ul>
            <div class="bordered-container">
            <div class="b-dics" style="width: 60%; font-weight: 600; margin: 0 auto;">
                <img src="assets/rnb_neus/images/cmp_neus/buddha/neus_5.png" alt="NeuS">
                <img src="assets/rnb_neus/images/cmp_neus/buddha/ours_5.png" alt="RNb-Neus (Ours)">
            </div>
            </div>
        </div>


        <!-- Relighting video -->
        <!-- <hr style="border-top: 3px double;"> -->
        <h6>Relighting with reflectance maps</h6>
        <p style="text-align: justify;">
            We provide videos demonstrating the relighting capabilities of our method as an additional result. 
            The videos show the 3D reconstruction of DiLiGenT-MV objects, with the reflectance, roughness and metalness maps used as texture under varying lighting direction.
        </p> 
        <div class="container;" style="padding-bottom: 10px; width: 70%; margin: 0 auto;">
            <ul class="nav nav-tabs nav-fill nav-justified" id="video-changer-2">   
                <li class="nav-item">
                    <a class="nav-link" onclick="changeVideo2(0)">Bear</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" onclick="changeVideo2(1)">Buddha</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" onclick="changeVideo2(2)">Cow</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link  active" onclick="changeVideo2(3)">Pot2</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" onclick="changeVideo2(4)">Reading</a>
                </li>
            </ul>
            
            <div>
                <video id="myVideo2" width="100%" class="approach_video" controls="" autoplay="" muted="" loop="">
                    <source  src="assets/rnb_neus/videos/relighting/pot2.mp4" type="video/mp4">
                </video>
            </div>
        </div>

  
        
        <!--
        <hr style="border-top: 3px double;">
        <h5>Approach</h5>
        <div class="container; padding-bottom: 10px;">
            <div>
                RNb-Neus builds on top of <a href="https://lingjie0206.github.io/papers/NeuS/">SDF-based volume rendering</a>.
            </div>

            <div>
                <br>
                <h6 style="margin-bottom: 30px;">1.&nbsp;&nbsp;Overview</h6>

                <p style="text-align: justify;"">
                    Given multi-view and multi-light images of an object acquired from N sparse views, 
                    our proposed methodology employs a two-step process. Initially, it utilizes a Photometric 
                    Stereo (PS) solution—specifically, SDM-UniPS as detailed in our paper—to derive reflectance 
                    and albedo maps. Subsequently, we leverage this information to generate simulated data adhering 
                    to a purely Lambertian model. The subsequent step involves the application of the NeuS architecture.
                    This architecture facilitates volume rendering employing a Lambertian approach by utilizing the 
                    normals of the Neural Signed Distance Function (SDF) alongside a network that predicts albedo at 
                    any spatial point. The optimization process involves minimizing the disparity between our simulated 
                    images and the rendered images, ultimately yielding a refined Neural SDF that accurately represents 
                    the object under consideration.
                </p>
                
                <div class="text-center;"" style="padding-top: 30px; padding-bottom: 30px">
                    <image src="assets/rnb_neus/images/pipeline.png" class="img-responsive padding-top: 100px" alt="overview">
                </div>
            </div>
        </div>
		-->

        <!-- Citation -->
        <hr style="border-top: 3px double;">
        <h5>Citation</h5>
        <div class="row">
            <div id="citation">
                @inproceedings{Brument23,<br>
                &nbsp;&nbsp;title={RNb-Neus: Reflectance and normal Based reconstruction with NeuS},<br>
                &nbsp;&nbsp;author={Baptiste Brument and
                Robin Bruneau and
                Yvain Quéau and
                Jean Mélou and
                François Lauze and Jean-Denis Durou and
                Lilian Calvet},<br>
                &nbsp;&nbsp;eprint={2312.01215},<br>
                &nbsp;&nbsp;archivePrefix={arXiv},<br>
                <!--&nbsp;&nbsp;booktitle={IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})},<br>-->
                &nbsp;&nbsp;year={2023}<br>
                }
            </div> 
        </div>
        <div style="text-align:center;">
            <button id="copy-button" class="copy-button" onclick="copyTextToClipboard()" >Copy</button>
        </div>

        <!-- Acknowledgements -->
        <hr style="border-top: 3px double;">
        <h5>Acknowledgements</h5>
        <p style="text-align: justify;">
            Baptiste Brument's doctoral student fellowship is funded by the French Ministry of Higher Education and Research.
            Robin Bruneau's doctoral student fellowship is funded by the Danish project PHYLORAMA. This work was partly funded by the French National Research Agency through the LabCom project ALICIA-Vision.
            Lilian Calvet's posdoctoral fellowship is supported by the OR-X - a swiss national research infrastructure for translational surgery - and associated funding by the University of Zürich and University Hospital Balgrist.
        </p>
    </div>

<div id="copyMessage">
@inproceedings{Brument23,
    title={RNb-Neus: Reflectance and normal Based reconstruction with NeuS},
    author={Baptiste Brument and Robin Bruneau and Yvain Quéau and Jean Mélou and François Lauze and Jean-Denis Durou and Lilian Calvet},
    eprint={2312.01215},
    archivePrefix={arXiv},
    year={2023}
}
</div>

        <script>  
            function copyTextToClipboard() {
                // Get the text with line breaks
                const textWithLineBreaks = document.getElementById('copyMessage').innerHTML;

                // Copy the text to the clipboard
                navigator.clipboard.writeText(textWithLineBreaks)
                    .then(() => {
                        // Optionally, provide some feedback to the user
                        document.getElementById('copy-button').style.backgroundColor = 'green';
                        document.getElementById('copy-button').textContent = 'Copied';
                    })
                    .catch(err => {
                        console.error('Unable to copy text to clipboard', err);
                    });
                }
        </script>


    </div>

</body>

</html>